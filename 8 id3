import pandas as pd
from collections import Counter
import math
from pprint import pprint

# Entropy calculation function
def entropy(probs):
    return sum(-prob * math.log(prob, 2) for prob in probs if prob > 0)

def entropy_of_list(a_list):
    cnt = Counter(a_list)
    num_instances = len(a_list)
    probs = [x / num_instances for x in cnt.values()]
    return entropy(probs)

# Corrected Information Gain function
def information_gain(df, split_attribute_name, target_attribute_name):
    df_split = df.groupby(split_attribute_name)
    nobs = len(df.index) * 1.0
    df_agg_ent = df_split[target_attribute_name].agg([entropy_of_list, lambda x: len(x) / nobs])
    df_agg_ent.columns = ['entropy', 'prob']
    avg_info = sum(df_agg_ent['entropy'] * df_agg_ent['prob'])
    old_entropy = entropy_of_list(df[target_attribute_name])
    return old_entropy - avg_info

# ID3 Decision Tree algorithm
def id3DT(df, target_attribute_name, attribute_names, default_class=None):
    # Count the frequency of each target value
    cnt = Counter(df[target_attribute_name])

    # Base case: If all target values are the same, return the class
    if len(cnt) == 1:
        return next(iter(cnt))

    # Base case: If the dataset is empty or no attributes remain, return the default class
    elif df.empty or not attribute_names:
        return default_class

    # Recursive case
    else:
        # Set the default class as the most common target value
        default_class = max(cnt, key=cnt.get)

        # Calculate information gain for each attribute
        gain = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]

        # Select the attribute with the maximum information gain
        index_of_max = gain.index(max(gain))
        best_attr = attribute_names[index_of_max]

        # Create the root of the tree
        tree = {best_attr: {}}

        # Remove the selected attribute from the attribute list
        remaining_attributes = [i for i in attribute_names if i != best_attr]

        # Split the dataset by the best attribute and recurse
        for attr_val, data_subset in df.groupby(best_attr):
            subtree = id3DT(data_subset, target_attribute_name, remaining_attributes, default_class)
            tree[best_attr][attr_val] = subtree

        return tree

# Classification function
def classify(instance, tree, default=None):
    attribute = next(iter(tree))  # Get the root attribute
    if instance[attribute] in tree[attribute]:  # If the attribute value exists in the tree
        result = tree[attribute][instance[attribute]]
        if isinstance(result, dict):  # If the result is a subtree, recurse
            return classify(instance, result)
        else:
            return result  # Return the leaf node (class label)
    else:
        return default  # Return the default class if attribute value is not found

# Data
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

# Create a DataFrame
df = pd.DataFrame(data)

# Get attribute names
attribute_names = list(df.columns)
attribute_names.remove('PlayTennis')

# Build the decision tree
tree = id3DT(df, 'PlayTennis', attribute_names)
print("The Resultant Decision Tree is:")
pprint(tree)

# New data for prediction
new_data = {
    'Outlook': ['Rain', 'Sunny'],
    'Temperature': ['Mild', 'Hot'],
    'Humidity': ['High', 'Normal'],
    'Wind': ['Weak', 'Strong']
}
df2 = pd.DataFrame(new_data)

# Predict the new data
df2['Predicted'] = df2.apply(classify, axis=1, args=(tree, 'No'))
df2




0p
The Resultant Decision Tree is:
{'Outlook': {'Overcast': 'Yes',
             'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}},
             'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}
Outlook	Temperature	Humidity	Wind	Predicted
0	Rain	Mild	High	Weak	Yes
1	Sunny	Hot	Normal	Strong	Yes
